{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8935a4-770a-416b-b434-05911d3992aa",
   "metadata": {},
   "source": [
    "# JAX and its potential use to CMS\n",
    "\n",
    "### Sadie Bartholomew, CMS Away Day 2025\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02949b-4679-4231-82ad-07b272bda0aa",
   "metadata": {},
   "source": [
    "## 1. Summary of JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a79b7-8e75-460a-a776-e2ecb54a64f0",
   "metadata": {},
   "source": [
    "#### *“A Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning”*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741a3c2-6a5e-4a63-86be-2fd0db8d4b08",
   "metadata": {},
   "source": [
    "#### Quick links:\n",
    "\n",
    "* Docs: https://docs.jax.dev/\n",
    "* Codebase, issue tracker etc.: https://github.com/jax-ml/jax\n",
    "* 'Awesome' listing of myriad appliations: https://github.com/n2cholas/awesome-jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aff22e-ab28-4a0a-bfa1-67f4e74eecb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Key points:\n",
    "\n",
    "* Made by 'Big Tech' (mostly Google, some Nvidia) and relatively new to the scene - a \"nascent version\" of JAX was described in a 2018 paper.\n",
    "* The crux: *\"brings Autograd and XLA (Accelerated Linear Algebra) together\"* in order to provide *\"a differentiable Numpy that runs on accelerators\"*. Hence the name, **J**IT, **A**utoGrad, **X**LA.\n",
    "* Overall, offers means for parallel array-based computation especially targeting accelerators like GPUs and TPUs (tensor PUs), rather than traditional CPU based computing. Fast due to JIT + XLA.\n",
    "* Describes itself as *\"an extensible system for composable function transformations at scale\"* where *'composable function transformations'* are operations taking a function and returning a new one, that can be combined arbitrarily i.e. stacked, making it very functional in style.\n",
    "\n",
    "#### Comparison to similar tools:\n",
    "\n",
    "| Similar to: | ...such as: | ...in that: |\n",
    "| --- | --- | --- |\n",
    "| ML frameworks | PyTorch, TensorFlow, scikit-learn | it is often use to train neural networks. though has a more functional style for writing code in, as opposed to the ML frameworks which are more OO |\n",
    "| Efficient high-level languages | Julia | you write high-level code that runs like low-level code |\n",
    "| Array computing libraries | NumPy | it has a corresponding API (though extra APIs via 'layered API' available) |\n",
    "\n",
    "But *not* really similar to Dask, even though you might naively think so, because:\n",
    "\n",
    "* Dask targets (multi-core) CPUs or clusters for task-level parallelism, whereas JAX targets GPU and TPU parallelism, so they are designed to target pretty different cases.\n",
    "* Both are libraries focusing on efficiency, but JAX for compute-bound numeric computation (e.g. calculus, optimisation, etc.) whereas Dask is for data-bound data access, movement, and scheduling (e.g. filtering large datasets, out-of-core transforms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8221c7b6-6b34-4c10-be13-5d49c6d08577",
   "metadata": {},
   "source": [
    "#### Usage\n",
    "Installation from PyPI i.e. via `pip` but commands differ depending on the hardware you intend to use on. At its most basic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b8764f-7e06-4bb2-9fac-eafb5841ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "# help(jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9e348-7612-4d80-b605-43d7bf7c6e62",
   "metadata": {},
   "source": [
    "#### A layered API\n",
    "\n",
    "Has three levels of API available, depending on what you want. Possibly most notable is the NumPy-matched API, see https://docs.jax.dev/en/latest/jax.numpy.html (\"starting with JAX v0.4.32, jax.Array and jax.numpy are compatible with the Python Array API Standard\" https://data-apis.org/array-api/latest/ except with regard to in-place updates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6047f8f-8331-4c34-a144-57c746bb4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. jax.numpy is a high-level wrapper that provides a familiar interface. So instead of using:\n",
    "import numpy as np\n",
    "# you can do:\n",
    "import jax.numpy as jnp\n",
    "# help(jnp)\n",
    "# and use `jnp` instead of `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07ae2ae-6854-4493-8f90-4656ab353674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. jax.lax is a lower-level API that is stricter and often more powerful.\n",
    "# https://docs.jax.dev/en/latest/jax.lax.html\n",
    "from jax import lax\n",
    "# help(lax)\n",
    "# 3. All JAX operations are implemented in terms of operations in XLA – the Accelerated Linear Algebra compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0412035-3ebc-48bc-a13b-c23b738318dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n",
      " 0.66666667 0.77777778 0.88888889 1.        ]\n",
      "[0.         0.11111111 0.22222222 0.33333334 0.44444445 0.5555556\n",
      " 0.6666667  0.7777778  0.8888889  1.        ]\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# A quick explore of the numpy-like API\n",
    "\n",
    "# Create equivalent arrays\n",
    "np_array = np.linspace(0, 1, 10)\n",
    "print(np_array)\n",
    "jnp_array = jnp.linspace(0, 1, 10)\n",
    "print(jnp_array)\n",
    "\n",
    "# Check equality - depending on what you mean...\n",
    "print(np_array == jnp_array)\n",
    "print(np.array_equal(np_array, jnp_array))\n",
    "print(jnp.array_equal(np_array, jnp_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc8d6b7-d2ed-4b82-ad01-e2a61f6a38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.          0.11111111  0.22222222  0.33333333  0.44444444  0.55555556\n",
      "  0.66666667  0.77777778  0.88888889  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Warning! A key difference is that JAX arrays are always immutable, unlike numpy where you can change them e.g:\n",
    "np_array[0] = -1\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e361b09d-c04b-4a9b-a0e7-c043f9b60577",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Won't work with JAX arrays!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mjnp_array\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cf-env-312-numpy2/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:596\u001b[0m, in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unimplemented_setitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, x):\n\u001b[1;32m    593\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX arrays are immutable and do not support in-place item assignment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    594\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 596\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: JAX arrays are immutable and do not support in-place item assignment. Instead of x[idx] = y, use x = x.at[idx].set(y) or another .at[] method: https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "# Won't work with JAX arrays!\n",
    "jnp_array[0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8feb5bba-6850-4188-a486-91d05f82ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.          0.11111111  0.22222222  0.33333334  0.44444445  0.5555556\n",
      "  0.6666667   0.7777778   0.8888889   1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Following the advice from the traceback, note you can do what you wanted (though it requires creation of\n",
    "# a new array object in a functional array update) via:\n",
    "jnp_array = jnp_array.at[0].set(-1)\n",
    "print(jnp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0650324-7c5d-42b0-9146-cd70ee4f5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jax.scipy in jax:\n",
      "\n",
      "NAME\n",
      "    jax.scipy\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright 2018 The JAX Authors.\n",
      "    #\n",
      "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "    # you may not use this file except in compliance with the License.\n",
      "    # You may obtain a copy of the License at\n",
      "    #\n",
      "    #     https://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    cluster (package)\n",
      "    fft\n",
      "    integrate\n",
      "    interpolate (package)\n",
      "    linalg\n",
      "    ndimage\n",
      "    optimize (package)\n",
      "    signal\n",
      "    sparse (package)\n",
      "    spatial (package)\n",
      "    special\n",
      "    stats (package)\n",
      "\n",
      "FILE\n",
      "    /home/slb93/miniconda3/envs/cf-env-312-numpy2/lib/python3.12/site-packages/jax/scipy/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similar wrapper for SciPy! JAX-based implementations of SciPy API likewise is:\n",
    "# https://docs.jax.dev/en/latest/jax.scipy.html\n",
    "import jax.scipy as jscipy\n",
    "help(jscipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a5a8a-d14d-43b3-a53a-146aa5d46339",
   "metadata": {},
   "source": [
    "#### Short basic examples\n",
    "\n",
    "#### A. Auto-diff with e.g. `jax.grad` and linear algebra with `jax.linalg`\n",
    "\n",
    "Typical applications of JAX in primitive form - some automatic differentation and linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797587f-dc6b-43db-9f48-ef786ef81c0e",
   "metadata": {},
   "source": [
    "##### A1. Basic scalar example, finding a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f72858-1e7a-4dc3-92d0-8307370d61fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    }
   ],
   "source": [
    "def scalar_fn(x):\n",
    "    return x**3  # scalar output\n",
    "\n",
    "grad_f = jax.grad(scalar_fn)  # 1st derivative: 3*x^2\n",
    "second_grad_f = jax.grad(grad_f)  # 2nd derivative: 6*x\n",
    "\n",
    "x = 2.0\n",
    "print(second_grad_f(x))  # prints 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37f66d-d911-40ea-8824-8195065a009a",
   "metadata": {},
   "source": [
    "##### A2. Vector examples: define a matrix A and vector b to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51eb23f-4949-44eb-8fe8-d51b989e2819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x is: [0.8000001 2.3      ]\n",
      "First derivative shape: (2, 2)\n",
      "First derivative:\n",
      " [[-0.32000005 -0.9200001 ]\n",
      " [ 0.16000003  0.46000004]]\n",
      "Second derivative shape: (2, 2)\n",
      "Second derivative (gradient of summed gradient):\n",
      " [[ 0.29600003  0.38600004]\n",
      " [-0.10800001 -0.07800002]]\n"
     ]
    }
   ],
   "source": [
    "A = jnp.array([[3.0, 2.0], [1.0, 4.0]])\n",
    "b = jnp.array([7.0, 10.0])\n",
    "\n",
    "# Solve the linear system Ax = b\n",
    "x = jnp.linalg.solve(A, b)\n",
    "print(\"Solution x is:\", x)\n",
    "\n",
    "# Define function that returns a scalar first element of Ax = b solution, similar to above\n",
    "def matrix_fn(A):\n",
    "    return jnp.linalg.solve(A, b)[0]  # scalar output\n",
    "\n",
    "# First derivative: grad of f\n",
    "grad_f = jax.grad(matrix_fn)\n",
    "\n",
    "# Second derivative: grad of the sum of grad_f outputs (to reduce to scalar)\n",
    "second_grad_f = jax.grad(lambda A: jnp.sum(grad_f(A)))\n",
    "\n",
    "grad_val = grad_f(A)\n",
    "second_grad_val = second_grad_f(A)\n",
    "\n",
    "print(\"First derivative shape:\", grad_val.shape)  # Should be (2, 2)\n",
    "print(\"First derivative:\\n\", grad_val)\n",
    "\n",
    "print(\"Second derivative shape:\", second_grad_val.shape)  # Also (2, 2) in this case\n",
    "print(\"Second derivative (gradient of summed gradient):\\n\", second_grad_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be343016-5c70-4b6d-b623-16479c064415",
   "metadata": {},
   "source": [
    "##### A3. Matrix operations and decompositions, using matrix A as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae82ea7d-445f-4f7d-b57a-c7162ffb09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A * A.T is:\n",
      " [[13. 11.]\n",
      " [11. 17.]]\n",
      "Inverse is:\n",
      " [[ 0.40000004 -0.20000002]\n",
      " [-0.10000001  0.3       ]]\n",
      "Determinant is:\n",
      " 10.0\n",
      "SVD values are:\n",
      " [[-0.64074737 -0.76775175]\n",
      " [-0.7677517   0.64074737]] [5.116672  1.9543954] [[-0.5257311 -0.8506508]\n",
      " [-0.8506508  0.5257311]]\n",
      "Eigenvalues are:\n",
      " [2.+0.j 5.+0.j]\n"
     ]
    }
   ],
   "source": [
    "#   Matrix multiplication\n",
    "C = jnp.matmul(A, A.T)\n",
    "print(\"A * A.T is:\\n\", C)\n",
    "#   Inverse\n",
    "A_inv = jnp.linalg.inv(A)\n",
    "print(\"Inverse is:\\n\", A_inv)\n",
    "#   Determinant\n",
    "det_A = jnp.linalg.det(A)\n",
    "print(\"Determinant is:\\n\", det_A)\n",
    "#   Singular Value Decomposition\n",
    "U, S, Vh = jnp.linalg.svd(A)\n",
    "print(\"SVD values are:\\n\", U, S, Vh)\n",
    "#   Eigenvalues\n",
    "eigvals = jnp.linalg.eigvals(A)\n",
    "print(\"Eigenvalues are:\\n\", eigvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2621a-af9c-44b2-94d4-c75ec45ea95b",
   "metadata": {},
   "source": [
    "##### A4. Adding JIT compilation to aim to speed it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50fd221-4410-4c97-999e-a96d160556d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def scalar_fn(x):\n",
    "    return x**3\n",
    "\n",
    "@jax.jit\n",
    "def matrix_fn(A):\n",
    "    return jnp.linalg.solve(A, b)[0]  # scalar output\n",
    "\n",
    "# Same outputs, potential speed-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a01ce4-473f-4c23-b0c0-c337cc3e5cfb",
   "metadata": {},
   "source": [
    "Machine learning is mostly, under-the-hood, calculus (as I understand it) therefore the above snippets should demonstrate to you how JAX can be used to streamline and facilitate ML, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcac249-6ee1-4fa9-8ce9-d63ea13b0f5f",
   "metadata": {},
   "source": [
    "### B. Auto-vectorisation with `vmap`\n",
    "\n",
    "Another key function is `jax.vmap` for automatic vectorisation. jax.vmap automatically vectorizes a function, letting you apply it in parallel across a batch of inputs — without writing explicit loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8026d-e1be-402c-a02b-eb8650998536",
   "metadata": {},
   "source": [
    "##### B1. Basic example - batch solving the Ax = b from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85703a5a-cbd7-48ca-a80a-8613df07ab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions for each b:\n",
      " [[0.8000001  2.3       ]\n",
      " [0.20000002 0.2       ]\n",
      " [0.6        1.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# Batch of b vectors (e.g., 3 different right-hand sides)\n",
    "B = jnp.array([\n",
    "    [7.0, 10.0],\n",
    "    [1.0, 1.0],\n",
    "    [4.0, 5.0]\n",
    "])\n",
    "\n",
    "# Function to solve Ax = b for fixed A\n",
    "def matrix_fn_A_fixed(b):\n",
    "    return jnp.linalg.solve(A, b)\n",
    "\n",
    "# Vectorize over the 0th axis of B\n",
    "batched_solve = jax.vmap(matrix_fn_A_fixed)\n",
    "\n",
    "X = batched_solve(B)  # Each row of X is a solution to Ax_i = b_i\n",
    "print(\"Solutions for each b:\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94aed59-a87e-4357-a199-98442a21ffc6",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 2. JAX for CMS 🤝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84771634-9f82-4ccd-b8b1-a9310a8b31a2",
   "metadata": {},
   "source": [
    "#### From the above, I suspect JAX may be useful to us in these contexts:\n",
    "\n",
    "* Designed for use on accelerated hardware, so anywhere we are doing work where we know the architecture will have GPUs (or TPUs)\n",
    "* But for CPU-only systems it can still be useful since it offers efficient automatic differentiation\n",
    "* As it is Python-based with no bindings or interfaces for other languages, it would be most suitable for use in Python codes we work with as opposed to anything in other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb7b79-2b4a-493f-abb6-d62a3e8bf057",
   "metadata": {},
   "source": [
    "#### More specifically...\n",
    "\n",
    "...it could potentially be used in Python code we are reponsible for or contribute to, as:\n",
    "\n",
    "* a direct replacement for NumPy operations where mutation isn't required, *assuming* correct context and sufficient testing is done to be confident it results in speed-up rather than possible slowing;\n",
    "* where efficient calculus and linear algebra operations on scalars, vectors or matrices/tensors are required;\n",
    "* for any ML training we might want to do.\n",
    "\n",
    "For instance, one concrete idea I had was that we could perhaps upgrade our methematical operation offerings in cf-python to:\n",
    "\n",
    "* make our calculus operations such as derivative, gradient, laplacian, div and curl methods more efficient;\n",
    "* add linear algebra data functionality - we know that folk would find this useful at least thinking towards use for ML and similar (thinking of David Case's CMS meeting talk a while back regarding PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c684e-6279-485f-9dc3-c5fbe4945279",
   "metadata": {},
   "source": [
    "*****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
